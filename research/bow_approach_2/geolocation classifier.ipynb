{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1. preprocessing tweets\n",
    "### load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets = 943\n",
      "Number of labels = 943\n",
      "\n",
      "Samples of data:\n",
      "Country = us \tTweet = @Addictd2Success thx u for following\n",
      "Country = us \tTweet = Let's just say, if I were to ever switch teams, Khalesi would be top of the list. #girlcrush\n",
      "Country = ph \tTweet = Taemin jonghyun!!! Your birits make me go~ http://t.co/le8z3dntlA\n",
      "Country = id \tTweet = depart.senior 👻 rapat perdana (with Nyayu, Anita, and 8 others at Ruang Aescullap FK Unsri Madang) — https://t.co/swRALlNkrQ\n",
      "Country = ph \tTweet = Done with internship with this pretty little lady!  (@ Metropolitan Medical Center w/ 3 others) [pic]: http://t.co/1qH61R1t5r\n",
      "Country = gb \tTweet = Wow just Boruc's clanger! Haha Sunday League stuff that, Giroud couldn't believe his luck! #clown\n",
      "Country = my \tTweet = I'm at Sushi Zanmai (Petaling Jaya, Selangor) w/ 5 others http://t.co/bcNobykZ\n",
      "Country = us \tTweet = Mega Fest!!!! Its going down🙏🙌  @BishopJakes\n",
      "Country = gb \tTweet = @EllexxxPharrell wow love the pic babe xx\n",
      "Country = us \tTweet = You have no clue how much you hurt me\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "data = json.load(open(\"data.json\"))\n",
    "for k, v in data.items():\n",
    "    x.append(k)\n",
    "    y.append(v)\n",
    "    \n",
    "print(\"Number of tweets =\", len(x))\n",
    "print(\"Number of labels =\", len(y))\n",
    "print(\"\\nSamples of data:\")\n",
    "for i in range(10):\n",
    "    print(\"Country =\", y[i], \"\\tTweet =\", x[i])\n",
    "\n",
    "assert(len(x) == 943)\n",
    "assert(len(y) == 943)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed tweets = 943\n",
      "Number of preprocessed labels = 943\n",
      "\n",
      "Samples of preprocessed data:\n",
      "Country = us \tTweet = {'@addictd2success': 1, 'thx': 1, 'u': 1, 'following': 1}\n",
      "Country = us \tTweet = {\"let's\": 1, 'say': 1, 'ever': 1, 'switch': 1, 'teams': 1, 'khalesi': 1, 'would': 1, 'top': 1, 'list': 1, '#girlcrush': 1}\n",
      "Country = ph \tTweet = {'taemin': 1, 'jonghyun': 1, 'birits': 1, 'make': 1, 'go': 1, 'http://t.co/le8z3dntla': 1}\n",
      "Country = id \tTweet = {'depart.senior': 1, 'rapat': 1, 'perdana': 1, 'nyayu': 1, 'anita': 1, 'others': 1, 'ruang': 1, 'aescullap': 1, 'fk': 1, 'unsri': 1, 'madang': 1, 'https://t.co/swrallnkrq': 1}\n",
      "Country = ph \tTweet = {'done': 1, 'internship': 1, 'pretty': 1, 'little': 1, 'lady': 1, 'metropolitan': 1, 'medical': 1, 'center': 1, 'w': 1, 'others': 1, 'pic': 1, 'http://t.co/1qh61r1t5r': 1}\n",
      "Country = gb \tTweet = {'wow': 1, \"boruc's\": 1, 'clanger': 1, 'haha': 1, 'sunday': 1, 'league': 1, 'stuff': 1, 'giroud': 1, 'believe': 1, 'luck': 1, '#clown': 1}\n",
      "Country = my \tTweet = {\"i'm\": 1, 'sushi': 1, 'zanmai': 1, 'petaling': 1, 'jaya': 1, 'selangor': 1, 'w': 1, 'others': 1, 'http://t.co/bcnobykz': 1}\n",
      "Country = us \tTweet = {'mega': 1, 'fest': 1, 'going': 1, '@bishopjakes': 1}\n",
      "Country = gb \tTweet = {'@ellexxxpharrell': 1, 'wow': 1, 'love': 1, 'pic': 1, 'babe': 1, 'xx': 1}\n",
      "Country = us \tTweet = {'clue': 1, 'much': 1, 'hurt': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english')) #note: stopwords are all in lowercase\n",
    "\n",
    "def preprocess_data(data, labels):\n",
    "    cleaned_tokens = []\n",
    "    cleaned_labels = []\n",
    "    \n",
    "    for string, label in zip(data, labels):\n",
    "        dic = {}\n",
    "        tokens = tt.tokenize(string)    # step1. tokenize a tweet data\n",
    "        for token in tokens:\n",
    "            token = token.lower()       # step2. lowercase words\n",
    "            \n",
    "            # step3 & step4. save words including any English alphabets and not belonging to stopwords \n",
    "            if (re.search(\"[a-z]+\",token) and not token in stopwords):   \n",
    "                dic[token] = dic.get(token,0) + 1  \n",
    "                \n",
    "        # save preprocessed tokens with their label if any token exists\n",
    "        if len(dic) > 0:\n",
    "            cleaned_tokens.append(dic)\n",
    "            cleaned_labels.append(label)\n",
    "    \n",
    "    return cleaned_tokens, cleaned_labels    \n",
    "\n",
    "x_processed, y_processed = preprocess_data(x, y)\n",
    "\n",
    "print(\"Number of preprocessed tweets =\", len(x_processed))\n",
    "print(\"Number of preprocessed labels =\", len(y_processed))\n",
    "print(\"\\nSamples of preprocessed data:\")\n",
    "for i in range(10):\n",
    "    print(\"Country =\", y_processed[i], \"\\tTweet =\", x_processed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "assert(len(x_processed) == len(y_processed))\n",
    "assert(len(x_processed) > 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2. text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# initialise the objects\n",
    "x_train, x_dev, x_test = None, None, None\n",
    "y_train, y_dev, y_test = None, None, None\n",
    "\n",
    "# split datasets into 70:15:15 with tha same distribution\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_processed, y_processed, test_size = 0.3, \\\n",
    "                                                    stratify = y_processed, random_state = 1)\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_temp, y_temp, test_size = 0.5, \\\n",
    "                                                stratify = y_temp, random_state = 1)\n",
    "\n",
    "# vectorize tokens \n",
    "vectorizer = DictVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_dev = vectorizer.transform(x_dev)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "MultinomialNB\n",
      "alpha 0.001 w/ accuracy 0.23404255319148937\n",
      "alpha 0.01 w/ accuracy 0.23404255319148937\n",
      "alpha 0.1 w/ accuracy 0.22695035460992907\n",
      "alpha 1.0 w/ accuracy 0.2198581560283688\n",
      "alpha 10.0 w/ accuracy 0.2127659574468085\n",
      "alpha 100.0 w/ accuracy 0.19148936170212766\n",
      "===========================\n",
      "LogisticRegression\n",
      "C 0.001 w/ accuracy 0.22695035460992907\n",
      "C 0.01 w/ accuracy 0.22695035460992907\n",
      "C 0.1 w/ accuracy 0.22695035460992907\n",
      "C 0.5 w/ accuracy 0.22695035460992907\n",
      "C 1 w/ accuracy 0.22695035460992907\n",
      "C 5 w/ accuracy 0.23404255319148937\n",
      "C 10 w/ accuracy 0.24822695035460993\n",
      "C 50 w/ accuracy 0.23404255319148937\n",
      "C 100 w/ accuracy 0.23404255319148937\n",
      "\n",
      "\n",
      "best classifier of mutinomial naive bayes model : MultinomialNB(alpha=0.01) with accuracy (0.234)\n",
      "best classifier of logistic regression model : LogisticRegression(C=10) with accuracy (0.248)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tune hyper-paramters for MultinomialNB \n",
    "best_acc_NB = 0    \n",
    "best_clfNB = None\n",
    "alphas = np.logspace(-3, 2, num = 6)  \n",
    "\n",
    "print(\"===========================\\nMultinomialNB\")\n",
    "for a in alphas:\n",
    "    clfNB = MultinomialNB(alpha = a)\n",
    "    clfNB.fit(x_train,y_train)\n",
    "    prediction_NB = clfNB.predict(x_dev)\n",
    "    acc_NB = accuracy_score(y_dev,prediction_NB)\n",
    "    print(\"alpha\", a, \"w/ accuracy\", acc_NB)\n",
    "    if acc_NB >= best_acc_NB:\n",
    "        best_acc_NB = acc_NB\n",
    "        best_clfNB = clfNB\n",
    "    \n",
    "# tune hyper-paramters for LogisticRegression\n",
    "best_acc_LR = 0      \n",
    "best_clfLR = None\n",
    "# solver = ['newton-cg', 'liblinear','lbfgs']   \n",
    "# penalty = ['none', 'l1', 'l2']\n",
    "C = [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100]    \n",
    "\n",
    "print(\"===========================\\nLogisticRegression\")\n",
    "for c in C:\n",
    "    clfLR = LogisticRegression(C = c)\n",
    "    clfLR.fit(x_train,y_train)\n",
    "    prediction_LR = clfLR.predict(x_dev)\n",
    "    acc_LR = accuracy_score(y_dev,prediction_LR)\n",
    "    print(\"C\", c, \"w/ accuracy\", acc_LR)\n",
    "    if acc_LR >= best_acc_LR:\n",
    "        best_acc_LR = acc_LR\n",
    "        best_clfLR = clfLR\n",
    "\n",
    "# print the optimal hyperparameters\n",
    "print(\"\\n\\nbest classifier of mutinomial naive bayes model : %s with accuracy (%.3f)\" \\\n",
    "      % (best_clfNB,best_acc_NB))                \n",
    "print(\"best classifier of logistic regression model : %s with accuracy (%.3f)\" \\\n",
    "      % (best_clfLR, best_acc_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance evaluation with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> MultinomialNB Model Results <<\n",
      "\n",
      "Accuracy : 0.289\n",
      "Macro Avg. F-score : 0.284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          au       0.24      0.27      0.25        15\n",
      "          ca       0.12      0.13      0.13        15\n",
      "          de       0.18      0.25      0.21         8\n",
      "          gb       0.28      0.33      0.30        15\n",
      "          id       0.60      0.20      0.30        15\n",
      "          my       0.33      0.47      0.39        15\n",
      "          ph       0.47      0.53      0.50        15\n",
      "          sg       0.40      0.29      0.33        14\n",
      "          us       0.21      0.20      0.21        15\n",
      "          za       0.23      0.20      0.21        15\n",
      "\n",
      "    accuracy                           0.29       142\n",
      "   macro avg       0.31      0.29      0.28       142\n",
      "weighted avg       0.31      0.29      0.29       142\n",
      "\n",
      "\n",
      ">> Linear Regression Model Results <<\n",
      "\n",
      "Accuracy : 0.331\n",
      "Macro Avg. F-score : 0.323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          au       0.60      0.20      0.30        15\n",
      "          ca       0.19      0.20      0.19        15\n",
      "          de       0.33      0.25      0.29         8\n",
      "          gb       0.31      0.27      0.29        15\n",
      "          id       0.43      0.67      0.53        15\n",
      "          my       0.23      0.33      0.27        15\n",
      "          ph       0.38      0.33      0.36        15\n",
      "          sg       0.50      0.29      0.36        14\n",
      "          us       0.12      0.13      0.13        15\n",
      "          za       0.45      0.60      0.51        15\n",
      "\n",
      "    accuracy                           0.33       142\n",
      "   macro avg       0.36      0.33      0.32       142\n",
      "weighted avg       0.36      0.33      0.32       142\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAccording to the result, LR model performs better with test dataset\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import library\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# fit the training data into the best Naive Bayes classifier and predict the label of test data\n",
    "best_clfNB.fit(x_train,y_train)          \n",
    "nb_prediction = best_clfNB.predict(x_test)\n",
    "\n",
    "print(\"\\n>> MultinomialNB Model Results <<\\n\")\n",
    "print(\"Accuracy :\", round(accuracy_score(y_test,nb_prediction), 3))\n",
    "print(\"Macro Avg. F-score :\", round(f1_score(y_test,nb_prediction, average='macro'), 3))\n",
    "print(classification_report(y_test, nb_prediction))\n",
    "\n",
    "\n",
    "# fit the training data into the best Logistic Regression classifier and predict the label of test data\n",
    "best_clfLR.fit(x_train,y_train)\n",
    "lr_prediction = best_clfLR.predict(x_test)\n",
    "       \n",
    "print(\"\\n>> Linear Regression Model Results <<\\n\")\n",
    "print(\"Accuracy :\", round(accuracy_score(y_test,lr_prediction), 3))\n",
    "print(\"Macro Avg. F-score :\", round(f1_score(y_test,lr_prediction, average='macro'),3))\n",
    "print(classification_report(y_test, lr_prediction))\n",
    "\n",
    "'''\n",
    "According to the result, LR model performs better with test dataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### further analysis: what is the most important features and their weights for each class for the two classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier = Logistic Regression\n",
      "\n",
      "Country = au\n",
      "#melbourne (1.846) literally (1.828) australia (1.719) little (1.609) melbourne (1.508) instagrams (1.480) spammed (1.480) #mtvhottest (1.470) https://t.co/7rcjjptvl7 (1.460) summerpoyi's (1.460) @mattjohnbond (1.256) geelong (1.256) freo (1.256) realsies (1.256) @shell_07 (1.256) #amazing (1.256) @darrencriss (1.212) valentine's (1.212) @chriscolfer (1.212) green (1.174) \n",
      "\n",
      "\n",
      "Country = ca\n",
      "@aliclarke (1.934) bed (1.882) thing (1.698) works (1.693) hate (1.691) let's (1.650) think (1.480) movies (1.439) xoxo (1.393) @cheetahbiatch (1.393) kids (1.358) next (1.310) finally (1.211) beautiful (1.185) @samanthasharris (1.174) really (1.168) found (1.131) awesome (1.129) manor (1.122) carlyle (1.122) \n",
      "\n",
      "\n",
      "Country = de\n",
      "@fabiomarabini (2.023) happened (1.916) https://t.co/df7ficsci3 (1.796) roseninsel (1.796) @selenagomez (1.512) http://t.co/airtaqn48v (1.512) germany (1.462) #utopia (1.436) jessica (1.436) https://t.co/brkwmsvzrb (1.436) gauting (1.436) #truegrip (1.436) hyde (1.436) done (1.385) love (1.289) https://t.co/i7j5pmd3mx (1.278) workout (1.278) could (1.242) adlib (1.240) https://t.co/psveyka5g3 (1.240) \n",
      "\n",
      "\n",
      "Country = gb\n",
      "x (2.321) well (2.183) prom (1.729) interesting (1.729) http://t.co/w0tjrah9y7 (1.729) this'll (1.729) xx (1.713) yeah (1.703) http://t.co/4yniyezb0n (1.484) many (1.423) modelling (1.422) http://t.co/giq4dl9jcq (1.422) new (1.415) plz (1.397) chelsea (1.327) @jackclaudereadi (1.324) holly (1.313) help (1.285) @skermo711 (1.263) #palacefansinthemorning (1.263) \n",
      "\n",
      "\n",
      "Country = id\n",
      "pic (2.454) http://t.co/0uccot9gdn (2.017) http://t.co/jjcyszki2y (1.878) @smandatas_261 (1.553) followback (1.553) @farid_nugrahaa (1.553) https://t.co/3sxyzl4crq (1.553) wkwk (1.544) selamat (1.523) @justinbieber (1.316) overboard (1.299) rt (1.242) https://t.co/vs4bfdcvdc (1.156) @brunomars (1.156) https://t.co/cxzfcg0hyi (1.150) http://t.co/fd2wjjnrus (1.142) mayang (1.142) h (1.142) http://t.co/qciitw5ihf (1.142) nikes (1.142) \n",
      "\n",
      "\n",
      "Country = my\n",
      "@poemporns (2.412) @juztakumi (1.804) thank (1.653) hunny (1.521) @aqmarnaimi_ (1.521) @markthewise (1.497) depressing (1.497) kl (1.494) johor (1.488) way (1.456) take (1.340) http://t.co/6qowzr40be (1.324) cave (1.324) best (1.317) #dearmind (1.308) hekhek (1.292) @_amewwa_ (1.292) care (1.255) congrats (1.245) http://t.co/utnamcqqx5 (1.244) \n",
      "\n",
      "\n",
      "Country = ph\n",
      "baby (2.280) @home (1.809) thankyou (1.685) @fuccyoudis2o9 (1.685) :d (1.469) mcdonald's (1.438) http://t.co/fznyhrrj (1.390) evening (1.341) playing (1.335) city (1.330) im (1.291) last (1.280) progression (1.233) @jamalwade_ (1.233) pray (1.233) reason (1.225) reco (1.224) http://t.co/h0dlmhdtbf (1.224) @abbylable (1.183) kg (1.140) \n",
      "\n",
      "\n",
      "Country = sg\n",
      "singapore (3.032) @ricadetiquez (1.975) yepz (1.628) @dolly07397340 (1.628) @sbuxindonesia (1.628) https://t.co/g0022mcf4u (1.628) @stylomiloteen (1.465) yeap (1.465) perfect (1.412) fitness (1.411) others (1.336) hang (1.326) @megaekaputri_ (1.318) http://t.co/u1sdmqdlmk (1.300) awards (1.300) work (1.278) @thebeatles (1.276) https://t.co/rwqvgoxm26 (1.276) food (1.272) songs (1.236) \n",
      "\n",
      "\n",
      "Country = us\n",
      "http://t.co/ykkwnmzsir (2.081) tonight (1.690) past (1.642) still (1.504) ahhahaha (1.468) @b_diddddy (1.468) @barackobama (1.443) clue (1.419) going (1.411) rick (1.383) #thewalkingdead (1.383) stay (1.354) beer (1.346) beginning (1.300) @rcpolar (1.259) testimony (1.259) @5sos (1.221) cinderella (1.221) http://t.co/wrgwr4nbwe (1.221) princess (1.221) \n",
      "\n",
      "\n",
      "Country = za\n",
      "https://t.co/2fsyuigben (1.933) akubabuze (1.685) @zimtweets (1.685) close (1.616) good (1.522) @spha_blose (1.521) drawing (1.486) goodnight (1.421) man (1.409) makes (1.392) @giftnana7 (1.380) journalist (1.380) sir (1.352) @enyawmm (1.352) africa (1.285) south (1.285) love (1.225) feel (1.196) amount (1.165) heal (1.165) \n",
      "\n",
      "Classifier = Naive Bayes\n",
      "\n",
      "Country = au\n",
      "i'm (-4.3) one (-4.6) melbourne (-4.8) even (-5.0) love (-5.0) australia (-5.0) #mtvhottest (-5.3) great (-5.3) victoria (-5.3) direction (-5.3) come (-5.3) little (-5.3) another (-5.7) green (-5.7) already (-5.7) say (-5.7) good (-5.7) look (-5.7) please (-5.7) anything (-5.7) \n",
      "\n",
      "\n",
      "Country = ca\n",
      "got (-5.0) like (-5.0) good (-5.0) one (-5.0) i'm (-5.0) thing (-5.0) next (-5.3) get (-5.3) going (-5.3) #photo (-5.3) new (-5.3) bed (-5.3) think (-5.3) today (-5.3) first (-5.3) happy (-5.7) morning (-5.7) mount (-5.7) @mt_dougrams (-5.7) without (-5.7) \n",
      "\n",
      "\n",
      "Country = de\n",
      "posted (-4.3) photo (-4.3) love (-4.3) i'm (-4.6) germany (-4.6) night (-5.0) please (-5.0) medinger (-5.0) des (-5.0) year (-5.0) come (-5.0) https://t.co/dpfrjtdarl (-5.0) #rosegold (-5.0) almost (-5.0) done (-5.0) could (-5.0) markus (-5.0) krefeld (-5.0) picture (-5.0) tages (-5.0) \n",
      "\n",
      "\n",
      "Country = gb\n",
      "x (-4.5) i'm (-4.7) got (-4.7) beat (-5.0) hope (-5.0) well (-5.0) yeah (-5.0) new (-5.0) sure (-5.2) xx (-5.2) fear (-5.2) i've (-5.2) know (-5.2) plz (-5.2) beautiful (-5.2) back (-5.2) one (-5.2) love (-5.2) next (-5.2) get (-5.2) \n",
      "\n",
      "\n",
      "Country = id\n",
      "pic (-4.0) rt (-4.4) i'm (-4.6) others (-4.8) wkwk (-4.8) hiks (-5.1) @justinbieber (-5.5) mikleo (-5.5) posted (-5.5) love (-5.5) j.co (-5.5) mcdonald's (-5.5) photo (-5.5) happy (-5.5) jawa (-5.5) coffee (-5.5) direction (-5.5) lasagna (-5.5) kerja (-5.5) hari (-5.5) \n",
      "\n",
      "\n",
      "Country = my\n",
      "i'm (-3.6) happy (-4.6) birthday (-4.9) w (-4.9) take (-4.9) aku (-4.9) thank (-4.9) jaya (-5.2) kuala (-5.2) care (-5.2) time (-5.2) lumpur (-5.2) petaling (-5.2) ni (-5.2) u (-5.2) cendol (-5.2) johor (-5.2) nk (-5.2) selangor (-5.2) best (-5.2) \n",
      "\n",
      "\n",
      "Country = ph\n",
      "follow (-4.3) please (-4.3) city (-4.7) good (-4.9) w (-4.9) like (-4.9) im (-4.9) sm (-4.9) make (-4.9) others (-4.9) manila (-4.9) i'm (-5.2) dreams (-5.2) come (-5.2) ka (-5.2) na (-5.2) true (-5.2) us (-5.2) mo (-5.2) evening (-5.2) \n",
      "\n",
      "\n",
      "Country = sg\n",
      "singapore (-3.9) i'm (-4.2) w (-4.5) others (-4.5) work (-4.8) fitness (-5.0) make (-5.0) resort (-5.4) come (-5.4) hang (-5.4) family (-5.4) thanks (-5.4) perfect (-5.4) @weichen_7 (-5.4) studios (-5.4) hotel (-5.4) week (-5.4) blk (-5.4) point (-5.4) universal (-5.4) \n",
      "\n",
      "\n",
      "Country = us\n",
      "i'm (-4.5) going (-4.7) like (-4.7) go (-4.9) tonight (-4.9) time (-4.9) beer (-5.2) can't (-5.2) even (-5.2) get (-5.2) hope (-5.2) happy (-5.2) u (-5.2) birthday (-5.2) still (-5.2) christmas (-5.6) night (-5.6) wanna (-5.6) really (-5.6) beginning (-5.6) \n",
      "\n",
      "\n",
      "Country = za\n",
      "happy (-4.6) good (-4.7) today (-5.0) day (-5.0) u (-5.0) love (-5.0) like (-5.0) feel (-5.3) got (-5.3) africa (-5.3) way (-5.3) thank (-5.3) next (-5.3) south (-5.3) made (-5.3) ur (-5.3) makes (-5.3) time (-5.3) god (-5.3) guys (-5.7) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOnly two countries, Australia(au) and Singapore(sg), have some redundant words observed in two different models. \\nThey are the words describing a name of geographic location, such as 'melbourne', 'australia' in au \\nand 'singapore' in sg. However, overall trends in all countries show that the top 20 important words are \\nsignificantly different in different models. In particular, the difference in RL model results is more obviously \\nobserved between different countries. Seemingly, NB cannot effectively differentiate the countries based on tweet. \\nWhich can explain the relatively lowe accuracy of NB. \\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the required data\n",
    "countries = best_clfLR.classes_\n",
    "coefficients = best_clfLR.coef_\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Classifier = Logistic Regression\")\n",
    "# print the top 20 words with the highest weight per class in LR model\n",
    "for i in range(len(countries)):\n",
    "    print(\"\\nCountry =\", countries[i])\n",
    "    coef = coefficients[i]\n",
    "    rank_indices = np.argsort(-coef)  # rank coefficients in descending order and return it's index\n",
    "    for j in range(20):               # print the matching feature and coefficient \n",
    "        index = rank_indices[j]\n",
    "        print(\"%s (%.3f)\" % (features[index], coef[index]), end = \" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "# get the required data\n",
    "countries = best_clfNB.classes_\n",
    "coefficients = best_clfNB.coef_\n",
    "\n",
    "# print the top 20 words with the highest weight per class in NB model\n",
    "print(\"Classifier = Naive Bayes\")\n",
    "for i in range(len(countries)):\n",
    "    print(\"\\nCountry =\", countries[i])\n",
    "    coef = coefficients[i]\n",
    "    rank_indices = np.argsort(-coef)  # rank coefficients in descending order and return it's index\n",
    "    for j in range(20):               # print the matching feature and coefficient \n",
    "        index = rank_indices[j]\n",
    "        print(\"%s (%.1f)\" % (features[index], coef[index]), end = \" \")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "'''\n",
    "Only two countries, Australia(au) and Singapore(sg), have some redundant words observed in two different models. \n",
    "They are the words describing a name of geographic location, such as 'melbourne', 'australia' in au \n",
    "and 'singapore' in sg. However, overall trends in all countries show that the top 20 important words are \n",
    "significantly different in different models. In particular, the difference in RL model results is more obviously \n",
    "observed between different countries. Seemingly, NB cannot effectively differentiate the countries based on tweet. \n",
    "Which can explain the relatively lowe accuracy of NB. \n",
    "'''    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional blocks for hashtags \n",
    "#### preprocessing hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hashtags = 425\n",
      "['#100percentpay', '#1stsundayofoctober', '#1yearofalmostisneverenough', '#2011prdctn', '#2015eebritishfilmacademyawards', '#2k16', '#2littlebirds', '#365picture', '#5sosacousticatlanta', '#5sosfam', '#8thannualpubcrawl', '#affsuzukicup', '#aflpowertigers', '#ahimacon14', '#aim20', '#airasia', '#allcity', '#alliswell', '#allwedoiscurls', '#amazing', '#anferneehardaway', '#ariona', '#art', '#arte', '#artwork', '#ashes', '#asian', '#asiangirl', '#askcrawford', '#askherforfback', '#askolly', '#asksteven', '#at', '#australia', '#awesome', '#awesomepict', '#barcelona', '#bart', '#bayofislands', '#beautiful', '#bedimages', '#bell', '#beringmy', '#bettybooppose', '#bff', '#big', '#bigbertha', '#bigbreakfast', '#blackhat', '#blessedmorethanicanimagine', '#blessedsunday', '#blogtourambiente', '#bluemountains', '#bonekachika', '#boomtaob', '#booyaa', '#bored', '#boredom', '#bradersisterhood', '#breaktime', '#breedingground', '#bringithomemy', '#brooksengland', '#burgers', '#butitsalsokindofaphone', '#bye', '#camera', '#canadaelections', '#cbb', '#cbcolympics', '#cctv', '#cdnpoli', '#celebritycrush', '#chargers', '#chocolate', '#ciosummit', '#cleansidewalk', '#clown', '#coffeespoonart', '#colts', '#confused', '#cornell', '#country', '#craftbeer', '#creative', '#crepes', '#cumannanya', '#danny4mayor', '#data', '#date', '#datingsiteforyou', '#dearmind', '#deed', '#delightful', '#dennisrodman', '#design', '#devacurl', '#die', '#difd', '#diner', '#dinner', '#dragoncon', '#dus', '#dynamounlock', '#earrings', '#eeeeeehhh', '#election2015', '#electriccircus2014', '#endomondo', '#engine', '#english', '#europapark', '#excitables', '#fabulous', '#factorycampus', '#fall', '#familydinner', '#ff', '#fire', '#flambees', '#flashback', '#fly', '#focusateneo', '#followher', '#followme', '#foodporn', '#fotografías', '#fotorus', '#four', '#freaks', '#friday', '#fridaynight', '#fried', '#friends', '#friendshipflow', '#fries', '#frozenyoghurt', '#fun', '#future', '#galaxy', '#getfreetattooaviciipasses', '#girl', '#girlcrush', '#girls', '#givesmehope', '#goingout', '#google', '#graduated', '#grafunkthiremepls', '#grammyfans', '#grandmarnier', '#greenfood', '#grilled', '#gudnytall', '#gunner', '#gym', '#handbuiltbicycle', '#happybirthdaysandarapark', '#happyfriday', '#harimaumalaya', '#hb60', '#hens', '#hippy', '#holiday', '#hollywoodmusicawards', '#homemadegranola', '#hometomama', '#hot', '#hungergames', '#hungry', '#icu', '#ididntchoosethestudentlifeitchoseme', '#iloveyou', '#imsobored', '#imsosore', '#innoretail', '#insanity', '#insightmedia', '#insightmediasingapore', '#instaframeplus', '#instagood', '#instalook', '#isibaya', '#javaboy', '#jed', '#jewelry', '#jo', '#jordaan', '#jrsurfboards', '#justshare', '#kllive', '#ladygaga', '#latepost', '#laugh', '#laundryservice', '#lazysunday', '#learningcommunties', '#lebedeintennis', '#letmesleep', '#lfc', '#lgbt', '#life', '#lipstickfree', '#littlemonsters', '#loadsoffun', '#lol', '#longranger', '#lotsoflove', '#love', '#lovers', '#lovethisgirl', '#lovevibes', '#lte', '#magazinesandtvscreens', '#magic', '#makeupfree', '#makingemuklajawabnya', '#mamajeanneandme', '#mancrush', '#march', '#massacreconspiracy', '#mauce', '#mavic', '#me', '#meetup', '#melbourne', '#michaelkors', '#mindfulness', '#mkmedi', '#mobile', '#morning', '#mountains', '#movies', '#mtlnewtech', '#mtvhottest', '#mushroom', '#music', '#mustfollow', '#mwc14', '#mybabyemilia', '#myfriendsarebetterthanyours', '#nced', '#ncga', '#ncpol', '#nevergetsold', '#new', '#newlooks', '#newrecord', '#nextlevel', '#nfl', '#nickryrie', '#nochillzone', '#nofilter', '#notersnew2014', '#notreally', '#np', '#nye', '#of', '#offtochurch', '#ohyeah', '#oilandgas', '#olah', '#on', '#oops', '#openspace', '#oscars', '#oui', '#palacefansinthemorning', '#panther', '#panthers', '#partyhardpartyy', '#pats', '#pechanga', '#penny', '#peperoni', '#peppermoney', '#photo', '#photoby', '#photography', '#pic', '#pll', '#pmattheashes', '#pop-up', '#positivity', '#potd', '#procrastination', '#promise', '#purplefriday', '#rainorshine', '#reachingyougpeople', '#realgoodbikes', '#redbull', '#retail', '#revfcwh', '#rippaulwalker', '#ritenow', '#rollercoaster', '#rollersmusicawards', '#rollsroyce', '#rose', '#rosegold', '#rt', '#rundude', '#ryanpurrdler', '#sad', '#safm', '#saints', '#samsung', '#sandwich', '#sarahm', '#saulbass', '#scifigeek', '#security', '#seniorbabysenior', '#sexy', '#sfgiants', '#sggirls', '#shakes', '#shakethatbooty', '#shellvpower', '#shenanigans', '#shopaholic', '#siberuang', '#silver', '#singapore', '#singlefighter', '#sinvsmas', '#skeemsaam', '#skullsearcher', '#sl', '#socal', '#sorrynotsorry', '#southampton', '#spafy', '#spicy', '#spider', '#squad', '#stampede2014', '#startupfest', '#startuphub', '#starving', '#statoftheday', '#stayfreshsaturdaydbn', '#stkilda', '#stop', '#stopcomplaining', '#strictlyus', '#studio', '#summer', '#sun', '#sunrise', '#sunshine', '#supremelaundry', '#surfshop', '#swedumtl', '#sycip', '#taintedlove', '#takeabreak', '#tcschleissheim', '#tdwpliveinkl', '#teamcanada', '#tennis', '#thaiexpress', '#thaifood', '#thankful', '#thankyoulord', '#thankyoupatients', '#thecolorrun2014', '#thedisadvantagesofakindle', '#them', '#thenext5goldenyears', '#thevoiceau', '#thewalkingdead', '#thomassabo', '#throwback', '#thursday', '#ticklish', '#toronto', '#tpcoach', '#transit', '#truegrip', '#tuesday', '#tune', '#turbine', '#txlege', '#ujackbastards', '#umg', '#uniexamonasaturday', '#universal', '#uptonogood', '#urbangardening', '#uss', '#usydhereicome', '#usydoweek', '#utopia', '#vanilla', '#vca', '#vegan', '#veganfood', '#vegetables', '#vegetarian', '#video', '#vma', '#voteonedirection', '#vsco', '#vscocam', '#walking', '#watch', '#weare90s', '#wearesocial', '#white', '#wings', '#wok', '#wood', '#work', '#workmates', '#world', '#worldcup2014', '#yellow', '#yiamas', '#ynwa', '#youtube', '#yummy', '#yws13', '#zweihandvollfarm']\n"
     ]
    }
   ],
   "source": [
    "# a function to collect all unique hashtags in the preprocess data\n",
    "def get_all_hashtags(data):\n",
    "    hashtags = set([])\n",
    "    for d in data:\n",
    "        for word, frequency in d.items():\n",
    "            if word.startswith(\"#\") and len(word) > 1:\n",
    "                hashtags.add(word)\n",
    "    return hashtags\n",
    "\n",
    "hashtags = get_all_hashtags(x_processed)\n",
    "print(\"Number of hashtags =\", len(hashtags))\n",
    "print(sorted(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.MaxMatch algorithm for hastags** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vanilla ['#', 'vanilla']\n",
      "#vca ['#', 'v', 'ca']\n",
      "#vegan ['#', 'vega', 'n']\n",
      "#veganfood ['#', 'vega', 'n', 'food']\n",
      "#vegetables ['#', 'vegetables']\n",
      "#vegetarian ['#', 'vegetarian']\n",
      "#video ['#', 'video']\n",
      "#vma ['#', 'v', 'ma']\n",
      "#voteonedirection ['#', 'vote', 'one', 'direction']\n",
      "#vsco ['#', 'vs', 'c', 'o']\n",
      "#vscocam ['#', 'vs', 'coca', 'm']\n",
      "#walking ['#', 'walking']\n",
      "#watch ['#', 'watch']\n",
      "#weare90s ['#', 'wear', 'e', '9', '0', 's']\n",
      "#wearesocial ['#', 'weares', 'o', 'c', 'i', 'al']\n",
      "#white ['#', 'white']\n",
      "#wings ['#', 'wings']\n",
      "#wok ['#', 'wo', 'k']\n",
      "#wood ['#', 'wood']\n",
      "#work ['#', 'work']\n",
      "#workmates ['#', 'work', 'mates']\n",
      "#world ['#', 'world']\n",
      "#worldcup2014 ['#', 'world', 'cup', '2', '0', '1', '4']\n",
      "#yellow ['#', 'yellow']\n",
      "#yiamas ['#', 'y', 'i', 'ama', 's']\n",
      "#ynwa ['#', 'yn', 'wa']\n",
      "#youtube ['#', 'you', 'tube']\n",
      "#yummy ['#', 'yummy']\n",
      "#yws13 ['#', 'y', 'ws', '1', '3']\n",
      "#zweihandvollfarm ['#', 'z', 'wei', 'hand', 'vol', 'l', 'farm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words()) #a list of words provided by NLTK\n",
    "words = set([ word.lower() for word in words ]) #lowercase all the words for better matching\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def tokenize_hashtags(hashtags):\n",
    "    hashtags_dic = {}       \n",
    "    for hashtag in hashtags:\n",
    "        tokens = []\n",
    "        length = len(hashtag)\n",
    "        e = length     # initial end index to search\n",
    "        s = 0          # initial start index to search\n",
    "    \n",
    "        # match the longest word starting from index s and ending at index e\n",
    "        while s < length:\n",
    "            word = hashtag[s:e]\n",
    "            if lemmatize(word) in words:  # if the lemmatized substring is in word, save the original word \n",
    "                tokens.append(word)\n",
    "                s += len(word)\n",
    "                e = length\n",
    "            else:       # otherwise, shorten the substring and match again by moving e towards s\n",
    "                e -= 1\n",
    "                if e == s:     # if no matches, save a single character as a token\n",
    "                    tokens.append(word)\n",
    "                    s += 1\n",
    "                    e = length\n",
    "                    \n",
    "        hashtags_dic[hashtag] = tokens    \n",
    "    return hashtags_dic    \n",
    "\n",
    "#tokenise hashtags with MaxMatch\n",
    "tokenized_hashtags = tokenize_hashtags(hashtags)\n",
    "\n",
    "#print results\n",
    "for k, v in sorted(tokenized_hashtags.items())[-30:]:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "assert(len(tokenized_hashtags) == len(hashtags))\n",
    "assert(tokenized_hashtags[\"#newrecord\"] == [\"#\", \"new\", \"record\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. reversed MaxMatch algorithm for hastags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vanilla ['#', 'vanilla']\n",
      "#vca ['#', 'v', 'ca']\n",
      "#vegan ['#', 'v', 'e', 'gan']\n",
      "#veganfood ['#', 'v', 'e', 'gan', 'food']\n",
      "#vegetables ['#', 'vegetables']\n",
      "#vegetarian ['#', 'vegetarian']\n",
      "#video ['#', 'video']\n",
      "#vma ['#', 'v', 'ma']\n",
      "#voteonedirection ['#', 'vote', 'one', 'direction']\n",
      "#vsco ['#', 'vs', 'c', 'o']\n",
      "#vscocam ['#', 'vs', 'c', 'o', 'cam']\n",
      "#walking ['#', 'walking']\n",
      "#watch ['#', 'watch']\n",
      "#weare90s ['#', 'we', 'are', '9', '0', 's']\n",
      "#wearesocial ['#', 'we', 'are', 'social']\n",
      "#white ['#', 'white']\n",
      "#wings ['#', 'wings']\n",
      "#wok ['#', 'w', 'ok']\n",
      "#wood ['#', 'wood']\n",
      "#work ['#', 'work']\n",
      "#workmates ['#', 'work', 'mates']\n",
      "#world ['#', 'world']\n",
      "#worldcup2014 ['#', 'world', 'cup', '2', '0', '1', '4']\n",
      "#yellow ['#', 'yellow']\n",
      "#yiamas ['#', 'y', 'i', 'a', 'mas']\n",
      "#ynwa ['#', 'yn', 'wa']\n",
      "#youtube ['#', 'you', 'tube']\n",
      "#yummy ['#', 'yummy']\n",
      "#yws13 ['#', 'y', 'ws', '1', '3']\n",
      "#zweihandvollfarm ['#', 'z', 'wei', 'hand', 'vol', 'l', 'farm']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_hashtags_rev(hashtags):\n",
    "    hashtags_rev_dic = {}       # create a dictionary to save tokens\n",
    "    for hashtag in hashtags:\n",
    "        tokens = []\n",
    "        length = len(hashtag)\n",
    "        e = length              # initial end index to search\n",
    "        s = 0                   # initial start index to search\n",
    "        \n",
    "         # match the longest word starting from index s and ending at index e\n",
    "        while s < length:\n",
    "            word = hashtag[s:e]\n",
    "            if lemmatize(word) in words:   # if the lemmatized substring is in word, save the original word \n",
    "                tokens.append(word)\n",
    "                s = 0\n",
    "                e -= len(word)\n",
    "            else:                # otherwise, shorten the substring and match again by moving s towards e\n",
    "                s += 1\n",
    "                if e == s:       # if no matches, save a single character as a token\n",
    "                    tokens.append(word)\n",
    "                    s = 0\n",
    "                    e -= 1\n",
    "                    \n",
    "        hashtags_rev_dic[hashtag] = tokens[::-1]      # save a reversed set of tokens in dictionary\n",
    "    return hashtags_rev_dic  \n",
    "\n",
    "    \n",
    "#tokenise hashtags with the reversed version of MaxMatch\n",
    "tokenized_hashtags_rev = tokenize_hashtags_rev(hashtags)\n",
    "\n",
    "#print results\n",
    "for k, v in sorted(tokenized_hashtags_rev.items())[-30:]:\n",
    "    print(k, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "assert(len(tokenized_hashtags_rev) == len(hashtags))\n",
    "assert(tokenized_hashtags_rev[\"#newrecord\"] == [\"#\", \"new\", \"record\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Comparison between MaxMatch and reversed MaxMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. #1stsundayofoctober\n",
      "MaxMatch = [#, 1, st, sunday, ofo, c, tobe, r]; LogProb = -92.7 \n",
      "Reversed MaxMatch = [#, 1, st, sunday, of, october]; LogProb = -58.7\n",
      "\n",
      "2. #1yearofalmostisneverenough\n",
      "MaxMatch = [#, 1, year, of, almost, is, never, enough]; LogProb = -60.9 \n",
      "Reversed MaxMatch = [#, 1, year, of, al, mos, tis, never, enough]; LogProb = -86.9\n",
      "\n",
      "3. #8thannualpubcrawl\n",
      "MaxMatch = [#, 8, than, nu, alp, u, b, crawl]; LogProb = -90.1 \n",
      "Reversed MaxMatch = [#, 8, th, annual, pub, crawl]; LogProb = -71.6\n",
      "\n",
      "4. #affsuzukicup\n",
      "MaxMatch = [#, a, f, fs, u, z, u, k, i, cup]; LogProb = -104.6 \n",
      "Reversed MaxMatch = [#, a, f, f, suz, u, k, i, cup]; LogProb = -91.4\n",
      "\n",
      "5. #ahimacon14\n",
      "MaxMatch = [#, ah, ima, con, 1, 4]; LogProb = -67.2 \n",
      "Reversed MaxMatch = [#, a, hi, macon, 1, 4]; LogProb = -58.8\n",
      "\n",
      "6. #alliswell\n",
      "MaxMatch = [#, all, is, well]; LogProb = -32.0 \n",
      "Reversed MaxMatch = [#, al, li, swell]; LogProb = -50.7\n",
      "\n",
      "7. #allwedoiscurls\n",
      "MaxMatch = [#, all, wed, o, is, curls]; LogProb = -61.7 \n",
      "Reversed MaxMatch = [#, all, w, edo, is, curls]; LogProb = -64.3\n",
      "\n",
      "8. #anferneehardaway\n",
      "MaxMatch = [#, an, fern, e, eh, ar, daw, ay]; LogProb = -97.2 \n",
      "Reversed MaxMatch = [#, an, f, er, nee, hard, away]; LogProb = -74.8\n",
      "\n",
      "9. #ariona\n",
      "MaxMatch = [#, arion, a]; LogProb = -32.0 \n",
      "Reversed MaxMatch = [#, ar, i, ona]; LogProb = -47.5\n",
      "\n",
      "10. #arte\n",
      "MaxMatch = [#, art, e]; LogProb = -33.0 \n",
      "Reversed MaxMatch = [#, ar, te]; LogProb = -42.0\n",
      "\n",
      "11. #askherforfback\n",
      "MaxMatch = [#, ask, her, for, f, back]; LogProb = -51.5 \n",
      "Reversed MaxMatch = [#, ask, her, f, orf, back]; LogProb = -60.7\n",
      "\n",
      "12. #askolly\n",
      "MaxMatch = [#, ask, o, l, ly]; LogProb = -59.0 \n",
      "Reversed MaxMatch = [#, as, kol, ly]; LogProb = -47.1\n",
      "\n",
      "13. #asksteven\n",
      "MaxMatch = [#, asks, te, v, en]; LogProb = -61.7 \n",
      "Reversed MaxMatch = [#, ask, steven]; LogProb = -37.2\n",
      "\n",
      "14. #bedimages\n",
      "MaxMatch = [#, bedim, ages]; LogProb = -38.1 \n",
      "Reversed MaxMatch = [#, bed, images]; LogProb = -33.5\n",
      "\n",
      "15. #beringmy\n",
      "MaxMatch = [#, beri, n, g, my]; LogProb = -56.4 \n",
      "Reversed MaxMatch = [#, be, ring, my]; LogProb = -36.2\n",
      "\n",
      "16. #bettybooppose\n",
      "MaxMatch = [#, betty, boo, p, pose]; LogProb = -61.1 \n",
      "Reversed MaxMatch = [#, betty, bo, oppose]; LogProb = -51.2\n",
      "\n",
      "17. #blackhat\n",
      "MaxMatch = [#, black, hat]; LogProb = -32.7 \n",
      "Reversed MaxMatch = [#, b, lac, khat]; LogProb = -51.4\n",
      "\n",
      "18. #blessedmorethanicanimagine\n",
      "MaxMatch = [#, blessed, more, than, i, can, imagine]; LogProb = -60.1 \n",
      "Reversed MaxMatch = [#, blessed, more, th, ani, can, imagine]; LogProb = -75.0\n",
      "\n",
      "19. #blessedsunday\n",
      "MaxMatch = [#, blesseds, un, day]; LogProb = -46.9 \n",
      "Reversed MaxMatch = [#, blessed, sunday]; LogProb = -34.8\n",
      "\n",
      "20. #blogtourambiente\n",
      "MaxMatch = [#, blo, g, tour, ambient, e]; LogProb = -73.7 \n",
      "Reversed MaxMatch = [#, b, log, tou, ram, bien, te]; LogProb = -89.1\n",
      "\n",
      "21. #booyaa\n",
      "MaxMatch = [#, boo, ya, a]; LogProb = -43.7 \n",
      "Reversed MaxMatch = [#, boo, y, aa]; LogProb = -52.8\n",
      "\n",
      "22. #bradersisterhood\n",
      "MaxMatch = [#, brad, ers, ist, er, hood]; LogProb = -80.6 \n",
      "Reversed MaxMatch = [#, brad, er, sisterhood]; LogProb = -55.3\n",
      "\n",
      "23. #bringithomemy\n",
      "MaxMatch = [#, bring, it, home, my]; LogProb = -42.4 \n",
      "Reversed MaxMatch = [#, brin, git, home, my]; LogProb = -54.9\n",
      "\n",
      "24. #brooksengland\n",
      "MaxMatch = [#, brooks, en, gland]; LogProb = -48.8 \n",
      "Reversed MaxMatch = [#, brook, sen, gland]; LogProb = -52.3\n",
      "\n",
      "25. #burgers\n",
      "MaxMatch = [#, burg, ers]; LogProb = -42.0 \n",
      "Reversed MaxMatch = [#, bur, gers]; LogProb = -42.0\n",
      "\n",
      "26. #butitsalsokindofaphone\n",
      "MaxMatch = [#, but, its, also, kind, of, a, phone]; LogProb = -58.9 \n",
      "Reversed MaxMatch = [#, bu, tits, also, kin, do, fa, phone]; LogProb = -91.7\n",
      "\n",
      "27. #cbcolympics\n",
      "MaxMatch = [#, c, b, coly, m, pics]; LogProb = -71.7 \n",
      "Reversed MaxMatch = [#, c, b, col, ym, pics]; LogProb = -74.6\n",
      "\n",
      "28. #cdnpoli\n",
      "MaxMatch = [#, c, d, n, pol, i]; LogProb = -63.1 \n",
      "Reversed MaxMatch = [#, c, d, n, po, li]; LogProb = -70.9\n",
      "\n",
      "29. #ciosummit\n",
      "MaxMatch = [#, c, ios, um, mi, t]; LogProb = -71.9 \n",
      "Reversed MaxMatch = [#, c, io, summit]; LogProb = -48.0\n",
      "\n",
      "30. #cleansidewalk\n",
      "MaxMatch = [#, cleans, ide, walk]; LogProb = -50.7 \n",
      "Reversed MaxMatch = [#, clean, sidewalk]; LogProb = -34.7\n",
      "\n",
      "31. #coffeespoonart\n",
      "MaxMatch = [#, coffees, poon, art]; LogProb = -50.7 \n",
      "Reversed MaxMatch = [#, coffee, spoon, art]; LogProb = -44.4\n",
      "\n",
      "32. #cornell\n",
      "MaxMatch = [#, cornel, l]; LogProb = -39.2 \n",
      "Reversed MaxMatch = [#, cor, nell]; LogProb = -41.3\n",
      "\n",
      "33. #cumannanya\n",
      "MaxMatch = [#, cum, anna, n, ya]; LogProb = -62.7 \n",
      "Reversed MaxMatch = [#, c, u, mannan, ya]; LogProb = -61.0\n",
      "\n",
      "34. #datingsiteforyou\n",
      "MaxMatch = [#, datings, it, e, for, you]; LogProb = -54.0 \n",
      "Reversed MaxMatch = [#, dating, site, for, you]; LogProb = -47.0\n",
      "\n",
      "35. #difd\n",
      "MaxMatch = [#, di, f, d]; LogProb = -45.3 \n",
      "Reversed MaxMatch = [#, d, if, d]; LogProb = -40.5\n",
      "\n",
      "36. #endomondo\n",
      "MaxMatch = [#, end, om, on, do]; LogProb = -48.0 \n",
      "Reversed MaxMatch = [#, en, do, mon, do]; LogProb = -52.8\n",
      "\n",
      "37. #excitables\n",
      "MaxMatch = [#, excitable, s]; LogProb = -38.9 \n",
      "Reversed MaxMatch = [#, ex, c, i, tables]; LogProb = -51.8\n",
      "\n",
      "38. #flambees\n",
      "MaxMatch = [#, flamb, e, es]; LogProb = -52.3 \n",
      "Reversed MaxMatch = [#, flam, bees]; LogProb = -39.2\n",
      "\n",
      "39. #focusateneo\n",
      "MaxMatch = [#, focus, aten, e, o]; LogProb = -59.3 \n",
      "Reversed MaxMatch = [#, fo, c, u, sate, neo]; LogProb = -76.6\n",
      "\n",
      "40. #foodporn\n",
      "MaxMatch = [#, food, po, r, n]; LogProb = -57.0 \n",
      "Reversed MaxMatch = [#, food, p, or, n]; LogProb = -48.7\n",
      "\n",
      "41. #fotografías\n",
      "MaxMatch = [#, fot, og, ra, f, í, as]; LogProb = -84.9 \n",
      "Reversed MaxMatch = [#, f, oto, gra, f, í, as]; LogProb = -81.9\n",
      "\n",
      "42. #fotorus\n",
      "MaxMatch = [#, fot, or, us]; LogProb = -41.2 \n",
      "Reversed MaxMatch = [#, fo, torus]; LogProb = -42.0\n",
      "\n",
      "43. #getfreetattooaviciipasses\n",
      "MaxMatch = [#, get, freet, at, too, a, vic, i, i, passes]; LogProb = -86.0 \n",
      "Reversed MaxMatch = [#, get, free, tattoo, a, vic, i, i, passes]; LogProb = -81.8\n",
      "\n",
      "44. #goingout\n",
      "MaxMatch = [#, going, out]; LogProb = -28.4 \n",
      "Reversed MaxMatch = [#, go, in, gout]; LogProb = -38.5\n",
      "\n",
      "45. #google\n",
      "MaxMatch = [#, goo, g, l, e]; LogProb = -60.7 \n",
      "Reversed MaxMatch = [#, go, ogle]; LogProb = -35.6\n",
      "\n",
      "46. #grammyfans\n",
      "MaxMatch = [#, gram, my, fans]; LogProb = -43.4 \n",
      "Reversed MaxMatch = [#, g, rammy, fans]; LogProb = -50.1\n",
      "\n",
      "47. #grandmarnier\n",
      "MaxMatch = [#, grandma, r, ni, er]; LogProb = -63.7 \n",
      "Reversed MaxMatch = [#, grand, m, arni, er]; LogProb = -63.3\n",
      "\n",
      "48. #happybirthdaysandarapark\n",
      "MaxMatch = [#, happy, birthdays, anda, rap, ark]; LogProb = -78.3 \n",
      "Reversed MaxMatch = [#, happy, birthday, sand, ara, park]; LogProb = -68.6\n",
      "\n",
      "49. #harimaumalaya\n",
      "MaxMatch = [#, ha, rima, um, ala, ya]; LogProb = -79.7 \n",
      "Reversed MaxMatch = [#, h, ar, i, mau, mala, ya]; LogProb = -84.7\n",
      "\n",
      "50. #hometomama\n",
      "MaxMatch = [#, home, toma, ma]; LogProb = -46.7 \n",
      "Reversed MaxMatch = [#, home, tom, ama]; LogProb = -44.9\n",
      "\n",
      "51. #ididntchoosethestudentlifeitchoseme\n",
      "MaxMatch = [#, id, id, n, tch, o, ose, the, student, life, itch, ose, me]; LogProb = -143.7 \n",
      "Reversed MaxMatch = [#, i, didnt, choose, the, student, life, it, cho, seme]; LogProb = -95.9\n",
      "\n",
      "52. #imsobored\n",
      "MaxMatch = [#, i, ms, o, bored]; LogProb = -54.4 \n",
      "Reversed MaxMatch = [#, i, m, so, bored]; LogProb = -48.4\n",
      "\n",
      "53. #imsosore\n",
      "MaxMatch = [#, i, ms, os, ore]; LogProb = -59.7 \n",
      "Reversed MaxMatch = [#, i, m, so, sore]; LogProb = -48.7\n",
      "\n",
      "54. #innoretail\n",
      "MaxMatch = [#, inn, ore, tail]; LogProb = -50.5 \n",
      "Reversed MaxMatch = [#, in, no, retail]; LogProb = -35.3\n",
      "\n",
      "55. #insightmediasingapore\n",
      "MaxMatch = [#, insight, media, sing, a, pore]; LogProb = -63.6 \n",
      "Reversed MaxMatch = [#, insight, me, di, as, inga, pore]; LogProb = -74.7\n",
      "\n",
      "56. #instagood\n",
      "MaxMatch = [#, ins, tag, o, od]; LogProb = -64.9 \n",
      "Reversed MaxMatch = [#, ins, ta, good]; LogProb = -49.3\n",
      "\n",
      "57. #instalook\n",
      "MaxMatch = [#, ins, tal, o, ok]; LogProb = -64.9 \n",
      "Reversed MaxMatch = [#, ins, ta, look]; LogProb = -50.0\n",
      "\n",
      "58. #isibaya\n",
      "MaxMatch = [#, is, iba, ya]; LogProb = -45.2 \n",
      "Reversed MaxMatch = [#, i, si, baya]; LogProb = -46.8\n",
      "\n",
      "59. #jordaan\n",
      "MaxMatch = [#, jo, r, da, an]; LogProb = -53.4 \n",
      "Reversed MaxMatch = [#, j, or, da, an]; LogProb = -48.7\n",
      "\n",
      "60. #jrsurfboards\n",
      "MaxMatch = [#, j, rs, urf, boards]; LogProb = -64.2 \n",
      "Reversed MaxMatch = [#, j, r, surfboards]; LogProb = -50.4\n",
      "\n",
      "61. #ladygaga\n",
      "MaxMatch = [#, lady, gag, a]; LogProb = -40.0 \n",
      "Reversed MaxMatch = [#, lady, g, aga]; LogProb = -48.8\n",
      "\n",
      "62. #laundryservice\n",
      "MaxMatch = [#, laundrys, er, vice]; LogProb = -52.3 \n",
      "Reversed MaxMatch = [#, laundry, service]; LogProb = -34.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "63. #learningcommunties\n",
      "MaxMatch = [#, learning, c, om, munt, ies]; LogProb = -75.1 \n",
      "Reversed MaxMatch = [#, learning, c, om, m, unties]; LogProb = -72.3\n",
      "\n",
      "64. #lebedeintennis\n",
      "MaxMatch = [#, l, e, bed, e, in, tennis]; LogProb = -70.2 \n",
      "Reversed MaxMatch = [#, l, e, be, de, in, tennis]; LogProb = -65.2\n",
      "\n",
      "65. #letmesleep\n",
      "MaxMatch = [#, let, mes, leep]; LogProb = -50.1 \n",
      "Reversed MaxMatch = [#, let, me, sleep]; LogProb = -38.8\n",
      "\n",
      "66. #loadsoffun\n",
      "MaxMatch = [#, loads, off, un]; LogProb = -44.5 \n",
      "Reversed MaxMatch = [#, loads, of, fun]; LogProb = -39.3\n",
      "\n",
      "67. #longranger\n",
      "MaxMatch = [#, long, ranger]; LogProb = -34.3 \n",
      "Reversed MaxMatch = [#, l, on, granger]; LogProb = -44.4\n",
      "\n",
      "68. #magazinesandtvscreens\n",
      "MaxMatch = [#, magazines, and, t, vs, creen, s]; LogProb = -75.4 \n",
      "Reversed MaxMatch = [#, magazine, sand, t, v, screens]; LogProb = -66.9\n",
      "\n",
      "69. #makeupfree\n",
      "MaxMatch = [#, make, up, free]; LogProb = -36.2 \n",
      "Reversed MaxMatch = [#, ma, keup, free]; LogProb = -47.5\n",
      "\n",
      "70. #mamajeanneandme\n",
      "MaxMatch = [#, mam, a, jeanne, and, me]; LogProb = -56.6 \n",
      "Reversed MaxMatch = [#, m, ama, jeanne, and, me]; LogProb = -63.2\n",
      "\n",
      "71. #meetup\n",
      "MaxMatch = [#, meet, up]; LogProb = -29.5 \n",
      "Reversed MaxMatch = [#, me, e, tup]; LogProb = -45.2\n",
      "\n",
      "72. #melbourne\n",
      "MaxMatch = [#, mel, bourn, e]; LogProb = -49.4 \n",
      "Reversed MaxMatch = [#, m, elb, our, ne]; LogProb = -58.1\n",
      "\n",
      "73. #mtlnewtech\n",
      "MaxMatch = [#, m, t, l, newt, e, c, h]; LogProb = -88.1 \n",
      "Reversed MaxMatch = [#, m, t, l, new, tech]; LogProb = -64.4\n",
      "\n",
      "74. #myfriendsarebetterthanyours\n",
      "MaxMatch = [#, my, friends, are, better, than, yours]; LogProb = -60.6 \n",
      "Reversed MaxMatch = [#, my, friend, sare, better, than, yours]; LogProb = -69.2\n",
      "\n",
      "75. #nced\n",
      "MaxMatch = [#, n, ce, d]; LogProb = -48.4 \n",
      "Reversed MaxMatch = [#, n, c, ed]; LogProb = -44.9\n",
      "\n",
      "76. #nevergetsold\n",
      "MaxMatch = [#, never, gets, old]; LogProb = -38.8 \n",
      "Reversed MaxMatch = [#, never, get, sold]; LogProb = -39.0\n",
      "\n",
      "77. #nickryrie\n",
      "MaxMatch = [#, nick, r, yr, ie]; LogProb = -63.1 \n",
      "Reversed MaxMatch = [#, nick, r, y, rie]; LogProb = -61.3\n",
      "\n",
      "78. #oilandgas\n",
      "MaxMatch = [#, oil, and, gas]; LogProb = -36.6 \n",
      "Reversed MaxMatch = [#, o, i, land, gas]; LogProb = -48.2\n",
      "\n",
      "79. #olah\n",
      "MaxMatch = [#, o, la, h]; LogProb = -45.6 \n",
      "Reversed MaxMatch = [#, o, l, ah]; LogProb = -46.8\n",
      "\n",
      "80. #openspace\n",
      "MaxMatch = [#, opens, pace]; LogProb = -35.4 \n",
      "Reversed MaxMatch = [#, open, space]; LogProb = -31.0\n",
      "\n",
      "81. #palacefansinthemorning\n",
      "MaxMatch = [#, palace, fans, in, them, or, ning]; LogProb = -65.5 \n",
      "Reversed MaxMatch = [#, palace, fan, sin, the, morning]; LogProb = -56.9\n",
      "\n",
      "82. #pechanga\n",
      "MaxMatch = [#, pech, an, ga]; LogProb = -47.1 \n",
      "Reversed MaxMatch = [#, p, e, changa]; LogProb = -48.0\n",
      "\n",
      "83. #peperoni\n",
      "MaxMatch = [#, pep, er, on, i]; LogProb = -52.7 \n",
      "Reversed MaxMatch = [#, pep, e, ro, ni]; LogProb = -66.3\n",
      "\n",
      "84. #photoby\n",
      "MaxMatch = [#, photo, by]; LogProb = -31.7 \n",
      "Reversed MaxMatch = [#, pho, toby]; LogProb = -42.0\n",
      "\n",
      "85. #pmattheashes\n",
      "MaxMatch = [#, p, matt, he, ashes]; LogProb = -53.2 \n",
      "Reversed MaxMatch = [#, p, mat, the, ashes]; LogProb = -50.8\n",
      "\n",
      "86. #potd\n",
      "MaxMatch = [#, pot, d]; LogProb = -34.7 \n",
      "Reversed MaxMatch = [#, po, td]; LogProb = -41.3\n",
      "\n",
      "87. #rainorshine\n",
      "MaxMatch = [#, rain, ors, hin, e]; LogProb = -62.1 \n",
      "Reversed MaxMatch = [#, r, ai, nor, shine]; LogProb = -58.6\n",
      "\n",
      "88. #reachingyougpeople\n",
      "MaxMatch = [#, reaching, you, g, people]; LogProb = -48.6 \n",
      "Reversed MaxMatch = [#, reaching, yo, ug, people]; LogProb = -59.5\n",
      "\n",
      "89. #ritenow\n",
      "MaxMatch = [#, rite, now]; LogProb = -32.6 \n",
      "Reversed MaxMatch = [#, rit, enow]; LogProb = -42.0\n",
      "\n",
      "90. #samsung\n",
      "MaxMatch = [#, sams, un, g]; LogProb = -50.6 \n",
      "Reversed MaxMatch = [#, sam, sung]; LogProb = -34.7\n",
      "\n",
      "91. #scifigeek\n",
      "MaxMatch = [#, s, c, if, i, geek]; LogProb = -59.9 \n",
      "Reversed MaxMatch = [#, s, c, i, fi, geek]; LogProb = -67.6\n",
      "\n",
      "92. #seniorbabysenior\n",
      "MaxMatch = [#, senior, babys, en, io, r]; LogProb = -74.1 \n",
      "Reversed MaxMatch = [#, senior, baby, senior]; LogProb = -44.8\n",
      "\n",
      "93. #shopaholic\n",
      "MaxMatch = [#, shop, aho, li, c]; LogProb = -61.1 \n",
      "Reversed MaxMatch = [#, sho, paho, li, c]; LogProb = -65.2\n",
      "\n",
      "94. #siberuang\n",
      "MaxMatch = [#, sib, er, uang]; LogProb = -56.0 \n",
      "Reversed MaxMatch = [#, si, ber, uang]; LogProb = -55.3\n",
      "\n",
      "95. #singapore\n",
      "MaxMatch = [#, sing, a, pore]; LogProb = -41.3 \n",
      "Reversed MaxMatch = [#, s, inga, pore]; LogProb = -51.8\n",
      "\n",
      "96. #skeemsaam\n",
      "MaxMatch = [#, skee, ms, aam]; LogProb = -54.9 \n",
      "Reversed MaxMatch = [#, s, k, e, ems, aam]; LogProb = -74.8\n",
      "\n",
      "97. #skullsearcher\n",
      "MaxMatch = [#, skulls, ear, che, r]; LogProb = -61.2 \n",
      "Reversed MaxMatch = [#, skull, searcher]; LogProb = -40.6\n",
      "\n",
      "98. #socal\n",
      "MaxMatch = [#, soc, al]; LogProb = -38.8 \n",
      "Reversed MaxMatch = [#, so, cal]; LogProb = -33.7\n",
      "\n",
      "99. #southampton\n",
      "MaxMatch = [#, south, am, p, ton]; LogProb = -52.1 \n",
      "Reversed MaxMatch = [#, s, out, ham, p, ton]; LogProb = -63.3\n",
      "\n",
      "100. #startupfest\n",
      "MaxMatch = [#, start, up, fest]; LogProb = -43.4 \n",
      "Reversed MaxMatch = [#, star, tup, fest]; LogProb = -52.8\n",
      "\n",
      "101. #startuphub\n",
      "MaxMatch = [#, start, up, hub]; LogProb = -41.0 \n",
      "Reversed MaxMatch = [#, star, tup, hub]; LogProb = -50.3\n",
      "\n",
      "102. #statoftheday\n",
      "MaxMatch = [#, st, at, oft, he, day]; LogProb = -59.1 \n",
      "Reversed MaxMatch = [#, s, tat, of, the, day]; LogProb = -52.0\n",
      "\n",
      "103. #strictlyus\n",
      "MaxMatch = [#, strictly, us]; LogProb = -32.0 \n",
      "Reversed MaxMatch = [#, strict, l, yus]; LogProb = -50.7\n",
      "\n",
      "104. #surfshop\n",
      "MaxMatch = [#, surfs, hop]; LogProb = -40.9 \n",
      "Reversed MaxMatch = [#, surf, shop]; LogProb = -37.2\n",
      "\n",
      "105. #swedumtl\n",
      "MaxMatch = [#, s, wed, um, t, l]; LogProb = -71.0 \n",
      "Reversed MaxMatch = [#, s, we, dum, t, l]; LogProb = -65.8\n",
      "\n",
      "106. #takeabreak\n",
      "MaxMatch = [#, take, ab, reak]; LogProb = -48.2 \n",
      "Reversed MaxMatch = [#, ta, kea, break]; LogProb = -51.5\n",
      "\n",
      "107. #thankyoulord\n",
      "MaxMatch = [#, thank, youl, or, d]; LogProb = -54.1 \n",
      "Reversed MaxMatch = [#, thank, you, lord]; LogProb = -39.8\n",
      "\n",
      "108. #thankyoupatients\n",
      "MaxMatch = [#, thank, youp, ati, en, ts]; LogProb = -78.4 \n",
      "Reversed MaxMatch = [#, thank, you, patients]; LogProb = -40.7\n",
      "\n",
      "109. #thenext5goldenyears\n",
      "MaxMatch = [#, then, ex, t, 5, golden, years]; LogProb = -69.8 \n",
      "Reversed MaxMatch = [#, the, next, 5, golden, years]; LogProb = -51.3\n",
      "\n",
      "110. #thevoiceau\n",
      "MaxMatch = [#, the, voice, a, u]; LogProb = -40.8 \n",
      "Reversed MaxMatch = [#, the, v, o, i, c, ea, u]; LogProb = -78.3\n",
      "\n",
      "111. #thewalkingdead\n",
      "MaxMatch = [#, thew, alk, ing, dead]; LogProb = -64.9 \n",
      "Reversed MaxMatch = [#, the, walking, dead]; LogProb = -35.7\n",
      "\n",
      "112. #toronto\n",
      "MaxMatch = [#, toro, n, to]; LogProb = -42.2 \n",
      "Reversed MaxMatch = [#, tor, onto]; LogProb = -37.9\n",
      "\n",
      "113. #txlege\n",
      "MaxMatch = [#, t, x, leg, e]; LogProb = -55.1 \n",
      "Reversed MaxMatch = [#, t, x, l, e, ge]; LogProb = -69.3\n",
      "\n",
      "114. #uniexamonasaturday\n",
      "MaxMatch = [#, unie, x, am, ona, saturday]; LogProb = -71.6 \n",
      "Reversed MaxMatch = [#, u, ni, ex, a, mona, saturday]; LogProb = -80.0\n",
      "\n",
      "115. #uptonogood\n",
      "MaxMatch = [#, up, ton, og, o, od]; LogProb = -70.6 \n",
      "Reversed MaxMatch = [#, up, to, no, good]; LogProb = -38.0\n",
      "\n",
      "116. #usydhereicome\n",
      "MaxMatch = [#, us, y, d, here, i, come]; LogProb = -64.2 \n",
      "Reversed MaxMatch = [#, u, syd, here, i, come]; LogProb = -59.8\n",
      "\n",
      "117. #usydoweek\n",
      "MaxMatch = [#, us, y, dow, e, e, k]; LogProb = -78.3 \n",
      "Reversed MaxMatch = [#, us, y, do, week]; LogProb = -48.9\n",
      "\n",
      "118. #vegan\n",
      "MaxMatch = [#, vega, n]; LogProb = -38.4 \n",
      "Reversed MaxMatch = [#, v, e, gan]; LogProb = -49.0\n",
      "\n",
      "119. #veganfood\n",
      "MaxMatch = [#, vega, n, food]; LogProb = -47.4 \n",
      "Reversed MaxMatch = [#, v, e, gan, food]; LogProb = -58.0\n",
      "\n",
      "120. #vscocam\n",
      "MaxMatch = [#, vs, coca, m]; LogProb = -51.6 \n",
      "Reversed MaxMatch = [#, vs, c, o, cam]; LogProb = -59.6\n",
      "\n",
      "121. #weare90s\n",
      "MaxMatch = [#, wear, e, 9, 0, s]; LogProb = -66.3 \n",
      "Reversed MaxMatch = [#, we, are, 9, 0, s]; LogProb = -57.4\n",
      "\n",
      "122. #wearesocial\n",
      "MaxMatch = [#, weares, o, c, i, al]; LogProb = -64.1 \n",
      "Reversed MaxMatch = [#, we, are, social]; LogProb = -33.8\n",
      "\n",
      "123. #wok\n",
      "MaxMatch = [#, wo, k]; LogProb = -39.6 \n",
      "Reversed MaxMatch = [#, w, ok]; LogProb = -38.4\n",
      "\n",
      "124. #yiamas\n",
      "MaxMatch = [#, y, i, ama, s]; LogProb = -55.9 \n",
      "Reversed MaxMatch = [#, y, i, a, mas]; LogProb = -49.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIn general, the lower probability of tokens makes more sense. \\nFor instances, [#, 1, st, sunday, of, october] (LogProb = -58.7) is \\nbetter than [#, 1, st, sunday, ofo, c, tobe, r] (LogProb = -92.7). \\nLikewise, [#, 1, year, of, almost, is, never, enough] (LogProb = -60.9) sounds much better than \\n[#, 1, year, of, al, mos, tis, never, enough] (LogProb = -86.9). \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "# get words from brown corpus\n",
    "brown_words = brown.words()\n",
    "brown_words = [word.lower() for word in brown_words]   #lowercase all the words in the corpus\n",
    "num_unique_words = len(set(brown_words))         # the number of unique words in the corpus\n",
    "total_frequency = len(brown_words)               # the total number or words in the corpus\n",
    "\n",
    "i = 0   # count the number of output\n",
    "\n",
    "# print any mismatch results between MaxMatch and reversed MaxMatch algorithms in alphabetic order\n",
    "for hashtag in sorted(tokenized_hashtags_rev.keys()):\n",
    "    maxMatch = tokenized_hashtags[hashtag]\n",
    "    rev_maxMatch = tokenized_hashtags_rev[hashtag]\n",
    "    if maxMatch != rev_maxMatch:\n",
    "        i += 1\n",
    "        maxMatch_p = []\n",
    "        rev_maxMatch_p = []\n",
    "\n",
    "        for token in maxMatch:\n",
    "            frequency = brown_words.count(token)     # get the frequency of a token from corpus\n",
    "            maxMatch_p.append(frequency)            \n",
    "        maxMatch_p = np.array(maxMatch_p)\n",
    "        # calculate probability using add-one smoothing\n",
    "        maxMatch_p = (maxMatch_p + 1)/ (total_frequency + 1*num_unique_words) \n",
    "        maxMatch_p = np.log(np.prod(maxMatch_p))    # get the log probability\n",
    "        \n",
    "        for token in rev_maxMatch:\n",
    "            frequency = brown_words.count(token)    # get the frequency of a token from corpus\n",
    "            rev_maxMatch_p.append(frequency)\n",
    "        rev_maxMatch_p = np.array(rev_maxMatch_p)\n",
    "        # calculate probability using add-one smoothing\n",
    "        rev_maxMatch_p = (rev_maxMatch_p + 1)/ (total_frequency + 1*num_unique_words)\n",
    "        rev_maxMatch_p = np.log(np.prod(rev_maxMatch_p))     # get the log probability\n",
    "        \n",
    "        print(\"\\n%d. %s\\nMaxMatch = [%s]; LogProb = %.1f \\nReversed MaxMatch = [%s]; LogProb = %.1f\" \\\n",
    "              % (i, hashtag, ', '.join(map(str, maxMatch)), maxMatch_p, \\\n",
    "                 ', '.join(map(str, rev_maxMatch)), rev_maxMatch_p))\n",
    "\n",
    "        \n",
    "'''\n",
    "In general, the lower probability of tokens makes more sense. \n",
    "For instances, [#, 1, st, sunday, of, october] (LogProb = -58.7) is \n",
    "better than [#, 1, st, sunday, ofo, c, tobe, r] (LogProb = -92.7). \n",
    "Likewise, [#, 1, year, of, almost, is, never, enough] (LogProb = -60.9) sounds much better than \n",
    "[#, 1, year, of, al, mos, tis, never, enough] (LogProb = -86.9). \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
